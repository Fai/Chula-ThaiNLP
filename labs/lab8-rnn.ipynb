{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Recurrent Neural Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb79c2d3cd469a77"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%pip install keras tensorflow"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebf61c1458222eb7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Input, TextVectorization, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Load a text corpus\n",
    "corpus = ['A A A A B A A B A A B B C A A B',\n",
    "          'A A B B B C A A B A A B B B A B B C A',\n",
    "          'A A A B A B B C A A'] * 1000\n",
    "\n",
    "split_point = len(corpus) * 4//5\n",
    "train_set = corpus[:split_point]\n",
    "dev_set = corpus[split_point:]\n",
    "\n",
    "vocab_size = 4\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "train_set = tokenizer.texts_to_sequences(train_set)\n",
    "dev_set = tokenizer.texts_to_sequences(dev_set)\n",
    "\n",
    "max_len = 20 \n",
    "train_set = pad_sequences(train_set, maxlen=max_len)\n",
    "dev_set = pad_sequences(dev_set, maxlen=max_len)\n",
    "\n",
    "def create_model(max_len, vocab_size, embedding_dim, hidden_dim):\n",
    "    model = Sequential()\n",
    "    # create the layer\n",
    "    model.add(Input(shape=(max_len-1,), dtype='int32'))\n",
    "    model.add(Embedding(vocab_size, embedding_dim, mask_zero=True))\n",
    "    model.add(SimpleRNN(hidden_dim, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = create_model(max_len, vocab_size, 100, 128)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c854dc074d9c61f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_set[:,:-1],\n",
    "    train_set[:,1:,None],\n",
    "    batch_size=64,\n",
    "    epochs=4,\n",
    "    validation_data=(dev_set[:,:-1], dev_set[:,1:,None]),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d0b7826a18ab6ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'dev'], loc='best')\n",
    "    plt.show()\n",
    "plot_history(history)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b99749a340decf00"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb8a7bbb48b1b1b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = 'B B'\n",
    "prompt_idx = tokenizer.texts_to_sequences([prompt])\n",
    "prompt_idx = pad_sequences(prompt_idx, maxlen=max_len-1)\n",
    "model.predict(prompt_idx)[0][-1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc2ed23ddea1853"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt_idx"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "217805f1454676ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.index_word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58d6f1cdc6437da5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy.random import choice\n",
    "\n",
    "def generate(prompt, tokenizer, predictor_model, end_token=None):\n",
    "    prompt_idx = tokenizer.texts_to_sequences([prompt])\n",
    "    prompt_idx = pad_sequences(prompt_idx, maxlen=max_len-1)\n",
    "    probs = predictor_model.predict(prompt_idx)[0][-1]\n",
    "    word_index = choice(a=len[probs], p=probs)\n",
    "    if word_index == 0 or (end_token is not None and word_index == tokenizer.word_index[end_token]):\n",
    "        return prompt\n",
    "    next_word = tokenizer.index_word[word_index]\n",
    "    prompt += ' ' + next_word\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c612aaa4e9f976d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generate('B B', tokenizer, model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1a0f579c49eb70b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using actual data, generate mixed English and Spanish names\n",
    "https://github.com/smashew/NameDatabases"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9245937fda2aaf0b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "names = [' '.join(list(x.lower().strip())) + ' end' for x in open('names.txt').readlines()]\n",
    "\n",
    "import random\n",
    "random.shuffle(names)\n",
    "split_point = len(names) * 9//10\n",
    "train_set = names[:split_point]\n",
    "dev_set = names[split_point:]\n",
    "\n",
    "vocab_size = 33\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(names)\n",
    "print(tokenizer.word_index)\n",
    "train_set = tokenizer.texts_to_sequences(train_set)\n",
    "dev_set = tokenizer.texts_to_sequences(dev_set)\n",
    "\n",
    "max_len = 20 # sequence length to pad the output to\n",
    "train_set = pad_sequences(train_set, maxlen=max_len)\n",
    "dev_set = pad_sequences(dev_set, maxlen=max_len)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2187d7b14c63e2f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "new_model = create_model(max_len, vocab_size, 40, 128)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "327c57c13f0e8e11"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "history = new_model.fit(\n",
    "    train_set[:,:-1],\n",
    "    train_set[:,1:,None],\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_data=(dev_set[:,:-1], dev_set[:,1:,None]),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea94ded0c536862b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_history(history)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23e13c708e55eba1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "new_name = generate('s t', tokenizer, new_model, end_token='end')\n",
    "new_name.capitalize().replace(' ','')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87c9fdc1b8bac9ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
